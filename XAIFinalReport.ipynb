{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a31c8d75",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Final Report\"\n",
    "subtitle: \"Interactive and Explainable AI\"\n",
    "author:\n",
    "  - name: \"Stijn But\"\n",
    "  - name: \"Minji Kim\"\n",
    "  - name: \"Xuechun Lyu\"\n",
    "  - name: \"Sercan Şeref\"\n",
    "date: \"21 April 2025\"\n",
    "bibliography: lib.bib\n",
    "abstract: |\n",
    "  This report presents the design, development, and evaluation of an interactive explainable AI (XAI) dashboard aimed at helping data science students—particularly first-time home buyers—interpret housing price predictions. Addressing the \"disagreement problem\" in XAI, the dashboard enables users to compare multiple explanation methods (SHAP, LIME, Integrated Gradients, SmoothGrad, and GradientShap) side-by-side, fostering transparency and trust in model outputs. The project combines user research, creative ideation, and iterative prototyping to deliver a tool that balances technical rigor with usability. Qualitative user testing highlights the dashboard’s effectiveness in clarifying feature importance and model behavior, while also identifying areas for further improvement in interpretability and user experience. The findings contribute practical insights for designing accessible XAI tools in real-world decision-making contexts.\n",
    "lang: \"en\"\n",
    "jupyter: python3 \n",
    "format:\n",
    "  pdf: \n",
    "    number-sections: true\n",
    "    colorlinks: true\n",
    "    documentclass: scrartcl\n",
    "    papersize: letter\n",
    "    toc: true\n",
    "code-line-numbers: true\n",
    "execute:\n",
    "  enabled: true\n",
    "  warning: true\n",
    "  error: true\n",
    "---\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66786d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "- *Description of the conducted assignment*\n",
    "\n",
    "The disagreement problem, as highlighted by Kaur et al. (2020), emphasizes the challenge of interpreting conflicting feature importances provided by different explanation tools and methods. This issue is particularly relevant in the context of dashboarding and explanation tools, where users often struggle with varying explanation styles and visualizations.\n",
    "\n",
    "Our project addresses this challenge by designing a dashboard specifically tailored for data science students. The dashboard aims to help users understand the features that contribute to housing price predictions by enabling them to compare multiple explanation methods side-by-side. This comparison provides insights into how different methods attribute importance and supports users in interpreting these explanations more effectively.\n",
    "\n",
    "The relevance of such a tool is particularly strong when considering the Dutch housing market, where housing prices have risen sharply in recent years. According to CBS (2024), the prices of existing homes are now higher than during the previous peak in 2008, with the pace of price increases slowing slightly around 2019 before accelerating again. In a market where finding affordable housing is increasingly challenging, a tool that explains housing price predictions in an accessible and transparent way can help users better understand the factors driving property prices and support more informed decision-making.\n",
    "\n",
    "We chose to focus on first-time house buyers, as they often face difficulties in understanding which features contribute most to a home's value. Given that first-time buyers are usually early in their careers and lack prior investment experience, they stand to benefit greatly from clear and interpretable AI explanations. Targeting data science students within this group was a deliberate choice, as their foundational knowledge of data concepts allows them to engage with and benefit from the explanations provided by the dashboard more effectively.\n",
    "\n",
    "- *Design debrief*\n",
    "\n",
    "To achieve this, we conducted user research and pilot testing using standard dashboarding tools. Based on the findings, we designed a prototype dashboard that aligns with the needs of our target audience. The datasets used for this project are sourced from the course materials or other relevant projects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822528f",
   "metadata": {},
   "source": [
    "# Empathize\n",
    "\n",
    "- *A well-argued and detailed description of the conducted pilot testing and qualitative user research methods (if any), review of XAI tools/techniques or literature*\n",
    "\n",
    "## Explanation Methods in Machine Learning\n",
    "\n",
    "We utilized various explanation methods introduced during the initial lectures and explored through the notebooks provided by the lecturers, including SHAP, LIME, Integrated Gradients, SmoothGrad, and GradientShap. These notebooks not only helped us understand the theoretical aspects of these methods but also demonstrated their practical applicability, showing that we could effectively use these techniques to address the housing problem in the Netherlands. Each of these methods offers unique approaches to understanding feature importance and model interpretability.\n",
    "\n",
    "The emergence of Explainable Artificial Intelligence (XAI) represents a critical development in addressing the opacity of complex machine learning models. Traditional predictive models, particularly those employed in high-stakes domains such as finance, healthcare, and housing economics, often suffer from a lack of interpretability. To bridge this gap, a variety of explanation techniques have been proposed, each offering different perspectives on how input features contribute to model outputs.\n",
    "\n",
    "Local Interpretable Model-Agnostic Explanations (LIME), introduced by Ribeiro et al. (2016), is a seminal contribution in this regard. LIME operates by approximating a complex model locally around a prediction using a simpler, interpretable surrogate model, often a linear regression. Through perturbing input data and observing output variations, LIME offers intuitive explanations that are particularly useful in understanding the behavior of highly non-linear models.[@ribeiro2016]\n",
    "\n",
    "Another important advancement is SHapley Additive exPlanations (SHAP), formulated by Lundberg and Lee (2017). Rooted in cooperative game theory, SHAP assigns each feature an importance value for a particular prediction by considering the contribution of features across all possible combinations. SHAP stands out due to its axiomatic foundation, guaranteeing properties such as local accuracy, consistency, and missingness, which are crucial for ensuring credible model interpretations.[@smilkov2017]\n",
    "\n",
    "Integrated Gradients, proposed by Sundararajan et al. (2017), takes a different approach, specifically designed for interpreting deep neural networks. This method attributes the change in prediction between a baseline and the actual input by integrating the gradients along a linear path. It satisfies important theoretical properties, such as sensitivity and implementation invariance, making it particularly suited for continuous and complex input spaces like images or tabular financial data.[@sundararajan2017]\n",
    "\n",
    "Another notable method is SmoothGrad, introduced by Smilkov et al. (2017), which improves the clarity of saliency maps by adding noise to the inputs and averaging the resulting gradients. Although initially proposed for visual data, adaptations of SmoothGrad to tabular data offer enhanced feature visualization by reducing noise and highlighting the regions of true importance.[@smilkov2017]\n",
    "\n",
    "Overall, these explanation methods each bring unique strengths. LIME offers model-agnostic, localized explanations ideal for exploratory analysis; SHAP provides a globally consistent, theoretically sound framework; Integrated Gradients excel in deep learning contexts; and SmoothGrad enhances the robustness and visual clarity of explanations. The synergy of these techniques creates a comprehensive interpretability toolkit essential for advancing transparent and trustworthy machine learning applications in various domains, including the housing market.\n",
    "\n",
    "## Applications of Explanation Methods to Housing Market Analysis\n",
    "\n",
    "The housing market has historically been analyzed through hedonic pricing models, wherein property characteristics such as location, size, and amenities are linked to price. However, with the advent of machine learning, more sophisticated models like Random Forests, XGBoost, and deep neural networks have demonstrated superior predictive capabilities. These advancements, while improving accuracy, have exacerbated concerns about model transparency, particularly in socially and economically sensitive sectors such as real estate.\n",
    "\n",
    "The application of XAI methods to housing market analysis addresses this issue by elucidating the underlying drivers of model predictions. For instance, Özçelik and Yildirim (2022) conducted a comparative study applying SHAP and LIME to real estate valuation models. Their findings consistently demonstrated that variables such as location proximity to urban centers, size of the dwelling, quality of neighborhood amenities, and macroeconomic indicators such as interest rates are the dominant predictors of property prices. Importantly, SHAP and LIME provided granular, instance-specific insights that enabled a deeper understanding of the multifaceted factors influencing real estate valuation. [@ozcelik2022]\n",
    "\n",
    "Feature importance analyses using SHAP and LIME have revealed recurrent patterns across various studies. Location factors, such as distance to city centers and accessibility to public transport, emerge as primary determinants of housing prices. Demographic variables, including median income levels and employment rates, also exhibit significant influence. Moreover, market dynamics such as housing supply-demand ratios and mortgage interest rates are critical economic indicators that affect property values. Physical attributes of houses, including size, number of rooms, age, and the presence of amenities such as gardens or parking spaces, consistently appear among the top predictors across diverse datasets.\n",
    "\n",
    "A particularly novel contribution to this field is the work by De Nadai et al. (2016), who utilized mobile phone activity data to quantify urban vitality, subsequently demonstrating its predictive power for housing price fluctuations. This study highlighted the potential of integrating unconventional datasets and features into traditional housing models, further underscoring the versatility of XAI methods in uncovering hidden patterns. [@denadai2016]\n",
    "\n",
    "Focusing specifically on the Dutch housing market, reports by Statistics Netherlands (CBS) and research conducted by Rabobank indicate distinct patterns that are critical for modeling efforts. Urbanization has driven significant price increases within the Randstad metropolitan region compared to rural provinces. Fluctuations in mortgage interest rates have shown a strong correlation with transaction volumes, emphasizing the sensitivity of the housing market to macroeconomic policy changes. Additionally, government interventions such as rent control policies and adjustments in mortgage lending standards have significantly influenced market dynamics. [@rabobank2024] [@cbs2024housing]\n",
    "\n",
    "In constructing educational dashboards aimed at data science students, the integration of explanation methods is particularly advantageous. Visual tools such as SHAP summary plots and LIME explanation graphs allow users to intuitively grasp the complex interplay of features driving housing market predictions. Furthermore, scenario simulation functionalities, wherein users can modify input features and observe corresponding changes in predictions, offer a hands-on understanding of model behavior. According to Molnar (2020), effective communication of explanations requires careful consideration of the audience’s domain knowledge, making simplicity, visual clarity, and contextual relevance crucial design principles. [@molnar2020]\n",
    "\n",
    "In conclusion, the integration of explanation methods into housing market analysis not only enhances model transparency but also facilitates a deeper comprehension of the economic, demographic, and physical factors shaping real estate dynamics. This synergy between advanced predictive modeling and interpretability is particularly valuable in educational contexts, equipping data science students with the necessary skills to build, critique, and trust predictive systems deployed in real-world scenarios.\n",
    "\n",
    "## User Research, Pilot Testing, and Results of Qualitative Analysis\n",
    "\n",
    "In addition to reviewing and testing various explanation methods, we conducted a focused user research study with data science students—our primary target audience—to evaluate the perceived value of the dashboard concept. The feedback gathered from these students strongly validated the need for such a tool. Participants consistently indicated that being able to compare different explanation methods side-by-side would significantly enhance their ability to understand and interpret model predictions. This input directly influenced the design and features of our prototype dashboard, ensuring it aligns with the needs and expectations of data science students who are considering buying their first house.\n",
    "\n",
    "### Summarize Key Findings from User Research\n",
    "\n",
    "The user research highlighted several important insights. The main challenge faced by students was understanding how housing price predictions are generated and which features most influence these predictions. The dashboard’s side-by-side comparison of explanation methods (such as SHAP and LIME) was seen as especially valuable, as it allowed users to easily observe both agreements and disagreements between methods. Participants also emphasized the usefulness of the top-5 feature comparison and the importance of clear, visually intuitive explanations. Overall, the research confirmed that such a dashboard meets a real need for transparency and interpretability in housing price prediction models, particularly for data science students preparing to make significant financial decisions.\n",
    "\n",
    "### Discuss Patterns or Themes\n",
    "\n",
    "A recurring theme was a preference for SHAP explanations, mainly due to their global perspective and visually intuitive plots. Visual clarity—such as color-coded graphs and clear labels—was repeatedly mentioned as crucial for comprehension. Some users noted confusion with certain LIME outputs and suggested that improved labeling and explanations would enhance usability. The side-by-side comparison feature was universally appreciated, as it directly addressed the “disagreement problem” and made the dashboard’s purpose clear.\n",
    "\n",
    "### Relate Findings to the Dashboard Design\n",
    "\n",
    "Feedback directly influenced the dashboard’s design: the side-by-side comparison module was prioritized, and efforts were made to improve visual clarity through color coding and concise labeling. Suggestions for further improvements, such as adding more descriptive titles and labels, were noted for future iterations. The dashboard’s layout and features were refined to better align with user needs for transparency and ease of interpretation.\n",
    "\n",
    "### Address Limitations\n",
    "\n",
    "The research was limited by a small and relatively homogeneous sample, primarily consisting of data science students. This focus aligns with our project's target audience—data science students who are considering buying their first house. While this ensures that the dashboard is well-tailored to users with some technical background, it may affect the generalizability of the findings to broader audiences, such as first-time home buyers without data science expertise. Additionally, some users found the explanations initially difficult to interpret, indicating a learning curve that could be mitigated with additional guidance or tutorials. Future iterations should consider expanding user testing to include a more diverse group to ensure broader accessibility and relevance.\n",
    "\n",
    "### Conclude with Validation\n",
    "\n",
    "Overall, the user research validated the need for the dashboard. Participants agreed that it addresses a real need for understanding housing price predictions and appreciated the transparency provided by feature importance explanations. While some skepticism remained regarding trust in the AI model—especially when different methods highlighted different features—the dashboard was seen as a valuable tool for interpreting and comparing model outputs.\n",
    "\n",
    "### Link to Future Work\n",
    "\n",
    "Findings suggest that future development should focus on enhancing visual clarity, adding user guidance, and possibly introducing interactive features such as a “build your own house” simulator. Broader user testing with more diverse groups is recommended to ensure accessibility and relevance beyond data science students. These steps will help refine the dashboard and maximize its impact for all users interested in explainable AI for housing price prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96a848",
   "metadata": {},
   "source": [
    "# Define\n",
    "\n",
    "- *A well-argued description of the XAI techniques and user needs chosen to focus on in the ideation phase*\n",
    "\n",
    "In the ideation phase, the XAI techniques selected were carefully chosen based on their potential to address the disagreement problem highlighted in the work of Kaur et al. (2020). This problem centers around the difficulty users face when interpreting conflicting feature importances produced by different explanation methods. To tackle this, the team focused on a combination of techniques known for their complementary strengths in offering transparent and interpretable explanations.\n",
    "\n",
    "One of the primary techniques selected was SHAP (SHapley Additive exPlanations), a game-theoretic approach that ensures consistency and fairness by considering all possible combinations of features. SHAP provides both global and local explanations, making it a versatile tool for understanding overall model behavior as well as individual predictions. In addition to SHAP, LIME (Local Interpretable Model-agnostic Explanations) was chosen for its ability to explain individual predictions through an interpretable surrogate model. Its model-agnostic nature and localized explanations made it particularly valuable for interpreting specific outputs.\n",
    "\n",
    "The team also selected several neural network-specific techniques, starting with Integrated Gradients, a method that computes feature attributions by integrating the gradients of the model’s output with respect to the input along a path from a baseline to the actual input. Integrated Gradients is especially effective for deep learning models, offering a clear and theoretically grounded attribution of input features. Building on this, SmoothGrad was incorporated to enhance the interpretability of gradient-based methods by reducing noise through the averaging of gradients over multiple noisy samples of the input. This leads to smoother and more comprehensible explanations. Finally, GradientShap was included as a variant of Integrated Gradients that combines the baseline approach with random sampling, averaging the attributions across multiple baselines to improve robustness and reliability.\n",
    "\n",
    "The user needs identified during the ideation phase were grounded in qualitative research conducted with data science students, which revealed several key priorities. First, users expressed a strong desire to be able to compare multiple explanation methods side-by-side, helping them to better interpret differences and similarities between explanations and to build greater trust in the AI models. Visual clarity emerged as another critical need, with users emphasizing the importance of intuitive, color-coded graphs and the provision of simplified visualizations for novices alongside more detailed plots for more advanced users.\n",
    "\n",
    "Beyond visual presentation, users highlighted the importance of accessibility and transparency. They needed brief, easy-to-understand explanations of each method to support their learning and ensure effective engagement with the dashboard, even for those without deep technical expertise. Interactivity was also seen as essential. Features like a \"What if?\" scenario explorer, trust meters to indicate confidence levels and data quality, and engaging animations were identified as ways to make the dashboard more dynamic, user-friendly, and supportive of deeper exploration.\n",
    "\n",
    "Lastly, the relevance of the dashboard to real-world problems was seen as a major factor in its perceived value. By focusing on housing price predictions—a topic highly pertinent to the Dutch housing market and the everyday concerns of young adults—the project aligned the technical goals of explainability with practical, real-life decision-making needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b35c9",
   "metadata": {},
   "source": [
    "# Ideation\n",
    "\n",
    "## Description of the creative techniques used for divergence and convergence\n",
    "\n",
    "- *Description of the creative techniques used for divergence and convergence*\n",
    "\n",
    "Throughout the project, creative thinking techniques were systematically applied during both the divergent and convergent phases to generate, explore, and refine ideas for the dashboard prototype.\n",
    "\n",
    "In the divergence phase, the team embraced an open brainstorming methodology based on the \"Yes, and...\" principle. This approach encouraged participants to freely build upon each other’s suggestions without immediate critique, fostering a highly creative atmosphere. As a result, a wide array of ideas was generated, ranging from practical enhancements to bold, innovative features. Ideas included integrating multiple explanation methods side-by-side, using color-coded graphs (e.g., red for negative and green for positive influences), implementing emojis and animations to make complex results more intuitive, and developing features like a “What if?” simulator and a trust meter.\n",
    "To support these creative efforts, team members also engaged in activities like wireframing dashboard layouts, sketching interaction flows, and using visual metaphors—such as likening integrated gradients to a “dimmer switch”—to make abstract AI explanations more relatable.\n",
    "\n",
    "Following the broad idea generation, the convergence phase was structured through a COCD Box (Creativity, Originality, Complexity, and Difficulty) framework. Here, ideas were categorized into Blue (feasible and easy to implement), Red (innovative and easy to implement), Yellow (innovative but harder to implement), and Grey (expensive or complex). This process enabled the team to filter the many brainstormed ideas and focus on those that offered the greatest impact relative to effort.\n",
    "Priority was given to feasible and innovative solutions such as the use of intuitive color schemes, brief textual explanations of each method, comparison modules between explanation techniques, playful visual enhancements (e.g., emojis and simple animations), and personalization options like simplified vs. detailed graphs for different users. The ideas from the Red and Blue zones were especially emphasized for rapid development, ensuring that the final product would be both creative and achievable within the project constraints.\n",
    "\n",
    "By combining free, expansive idea generation with structured selection and refinement, the team effectively balanced innovation and practicality, ensuring that the final dashboard would be both technically sound and genuinely user-centered.\n",
    "\n",
    "## Description of the chosen solution\n",
    "\n",
    "- *Result of the divergence and convergence technique - A well-argued description of the chosen solution*\n",
    "\n",
    "The application of divergence and convergence techniques directly resulted in the design and development of a dashboard that was both highly functional and distinctly user-friendly.\n",
    "\n",
    "One of the key outcomes was the explanation comparison module, which allowed users to view and compare feature attributions across multiple explanation methods, including SHAP, LIME, Integrated Gradients, and SmoothGrad. This feature directly addressed the disagreement problem (Kaur et al., 2020) by making it easier for users to observe where explanation methods agreed and where they diverged. As a result, users gained a clearer, multi-dimensional understanding of the AI model’s behavior rather than relying on a single method’s interpretation.\n",
    "\n",
    "The use of intuitive color coding helped users quickly grasp the meaning behind the feature importances. Meanwhile, features like the trust meter and concise method explanations addressed the user need for transparency and helped foster greater trust in the AI's outputs.\n",
    "\n",
    "Furthermore, by enabling users to toggle between simplified and advanced visualization modes, the dashboard successfully catered to the varying expertise levels of data science students who are first-time home buyers. This adaptability ensured that users, regardless of their technical proficiency, could effectively engage with the dashboard and gain valuable insights into housing price predictions.\n",
    "\n",
    "The final solution also remained highly relevant to real-world problems. Given the sharp rise in housing prices in the Netherlands (CBS, 2024), understanding the factors that drive property values has become increasingly important. The dashboard responded to this societal need by offering a practical, easy-to-use tool for interpreting housing price predictions, empowering users to make more informed financial decisions.\n",
    "\n",
    "In conclusion, the creative divergence ensured that a rich and varied pool of ideas was explored, while convergence ensured that the most impactful, user-centric, and feasible ideas were executed. The resulting dashboard is a testament to the power of structured creative thinking, offering a technically advanced yet highly intuitive solution to a real-world challenge in AI explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f30af5",
   "metadata": {},
   "source": [
    "# Prototype\n",
    "\n",
    "- *A well-argued description of the developed prototype, including images of the prototype*\n",
    "\n",
    "The developed prototype, implemented in python script, is an interactive, research-informed explainable AI (XAI) dashboard. It is designed to support data science students who are first-time home buyers in understanding and comparing how different machine learning models and interpretability techniques explain housing price predictions. The structure of the dashboard is directly guided by the findings of our user research and the XAI principles laid out in the literature.\n",
    "\n",
    "The dashboard is built with Streamlit, chosen for its ability to rapidly create web-based interactive applications. The visual interface is intuitive and accessible to users with varying technical expertise, allowing seamless navigation between models and explanation techniques. The dashboard integrates SHAP, LIME, and Captum libraries, providing users with multiple explanation methods that emphasize both transparency and flexibility. SHAP enables both global and local model explanations using a game-theoretic framework, LIME offers localized model-agnostic explanations, and Captum supports gradient-based interpretation methods for neural networks.\n",
    "\n",
    "The script starts by importing a comprehensive suite of packages essential for machine learning and explainability. In addition to data manipulation, and plotting, the script incorporates both tree-based (XGBoost) and neural network (PyTorch) models. It also supports explanation techniques across these models, enhancing comparative interpretability. The use of os, stats, and caching functions ensures efficient computation and robust statistical processing, including outlier detection.\n",
    "\n",
    "The core of the prototype lies in its dual-model architecture. A neural network is defined using PyTorch, consisting of four fully connected layers with ReLU activations and dropout layers for regularization. This model mirrors the input dimensionality of the dataset (30 features) and serves as a counterpart to the XGBoost regressor, which is trained directly on standardized training data. The inclusion of both models enables users to explore the strengths and limitations of distinct machine learning paradigms and their corresponding explanations.\n",
    "\n",
    "The data pipeline involves loading and cleaning a housing dataset. Non-binary features undergo outlier removal using z-score thresholds to ensure robust model behavior. Binary and non-binary features are recombined, and the resulting dataset is split into training and test sets. Standardization is applied to improve model convergence and ensure fair feature comparisons across explanation methods.\n",
    "\n",
    "Following model training, the dashboard loads or reuses a pre-trained neural network (if available), enhancing performance and reproducibility. It provides users the ability to select either model for explanation and choose individual samples from the test set to explore. This sample-specific analysis aligns with user-identified needs for local interpretability and contextual understanding of individual predictions.\n",
    "\n",
    "The user interface is structured around two primary display columns: one for SHAP and the other for LIME. For any selected sample, SHAP explanations are visualized using a waterfall plot to show individual feature contributions, and a summary plot that reveals overall feature importance across the test set. These visuals leverage clear, color-coded designs that were favored in user feedback sessions for their intuitive layout and interpretability.\n",
    "\n",
    "On the LIME side, the dashboard generates and embeds interactive HTML explanations for the selected prediction. These explanations detail how each feature influenced the model's decision, helping users trace the decision logic for individual predictions.\n",
    "\n",
    "When the neural network is selected, the dashboard additionally provides gradient-based explanations using Captum. It supports Integrated Gradients, GradientShap, and SmoothGrad, with bar plots displaying the top 10 features contributing to the model’s output. These methods offer deeper insights into neural model behavior, particularly valuable for users curious about how black-box models arrive at their predictions.\n",
    "\n",
    "The Comparison section synthesizes the top five features identified by SHAP and LIME, displaying them side by side and calculating feature overlap. This quantitative comparison helps users evaluate method agreement—a concern frequently cited in the XAI literature as the \"disagreement problem.\" By presenting agreement metrics and prediction values side by side, the dashboard promotes critical thinking and confidence in model interpretation.\n",
    "\n",
    "Finally, the dashboard includes a feedback section in the sidebar. Users can express which model and explanation method they found more understandable and trustworthy, and they are invited to share open-ended feedback. This mechanism supports iterative refinement of the tool, grounded in participatory design principles.\n",
    "\n",
    "Below are screenshots illustrating the dashboard’s core components:\n",
    "\n",
    "SHAP Waterfall and Summary Plots\n",
    "Displays individual and global feature contributions for the selected model.\n",
    "\n",
    "LIME Explanation Output\n",
    "Interactive HTML explanation of how features impacted a specific prediction.\n",
    "\n",
    "Captum-Based Gradient Explanations (Neural Network)\n",
    "Feature importance derived from Integrated Gradients, SmoothGrad, and GradientShap.\n",
    "\n",
    "Top Feature Comparison Between SHAP and LIME\n",
    "Table showing method agreement and differing perspectives on feature importance.\n",
    "\n",
    "Overall, the python script represents a thoughtful synthesis of explainability research, user feedback, and machine learning best practices. By integrating multiple models and explanation methods into one cohesive, interactive tool, the dashboard not only educates users about AI decision-making but also empowers them to compare, question, and trust the predictions. It transforms abstract XAI theory into a practical resource for informed housing decisions, especially for audiences navigating this complex domain for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de24b53",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "- *A well-argued and detailed description of the conducted qualitative user research methods used for testing the prototype.*\n",
    "- *Research question*\n",
    "- *Results of the data analysis*\n",
    "\n",
    "## Qualitative User Research Methods\n",
    "\n",
    "To evaluate the prototype dashboard, we conducted qualitative user research with data science students—our primary target audience—using a combination of structured user testing sessions and semi-structured feedback collection. Participants, consisting of both first-year and second-year master’s students in AI, Data Science, and Computer Science, interacted freely with the dashboard, exploring features such as side-by-side explanation comparisons (SHAP, LIME, gradient-based methods), model selection, and feature importance visualizations. After hands-on use, participants provided feedback via a structured table capturing their affiliation, explanation method preference, likes, criticisms, questions, improvement ideas, and general comments. This approach enabled us to gather both quantitative data (e.g., method preferences) and rich qualitative insights (e.g., usability, clarity, and suggestions). Open-ended questions focused on preferences, perceived clarity, difficulties, and suggestions for improvement, ensuring that the research addressed interpretability, usability, and overall effectiveness of the dashboard for users with moderate technical knowledge but varying familiarity with explainable AI (XAI) techniques.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "The central research question guiding our user testing was:  \n",
    "\n",
    "- **\"How do data science students perceive and interpret the explanations provided by different XAI methods in the dashboard, and what are their preferences, challenges, and suggestions for improving the interpretability and usability of housing price prediction models?\"**\n",
    "\n",
    "This question aimed to investigate whether the prototype could effectively support users in understanding the behavior of complex models and resolving discrepancies between different explanation outputs.\n",
    "\n",
    "## Results of the Data Analysis\n",
    "\n",
    "Analysis of user responses revealed several important trends. First, SHAP was the overwhelmingly preferred explanation method, praised for its clarity, intuitive visuals, and direct representation of feature influence (e.g., monetary values). Users described SHAP as easier to interpret and more informative, especially when comparing feature contributions.\n",
    "\n",
    "In contrast, LIME received mixed feedback. While some users found value in its localized explanations, others struggled with redundant or unclear visualizations. Comments frequently mentioned \"too many graphs,\" unclear terminology (such as \"sample index\"), and cognitive overload. There was a clear demand for better onboarding, including a brief user guide, tooltips, or an introductory tutorial explaining how each explanation method works and how to interpret the outputs.\n",
    "\n",
    "A recurring issue was confusion over the lack of overlap between explanation methods. Users often questioned which method to trust when SHAP, LIME, and gradient-based explanations disagreed. This confirmed the relevance of the disagreement problem (Kaur et al., 2020) and underscored the need for improved explanation harmonization or contextual guidance.\n",
    "\n",
    "Several usability issues were also identified. Participants noted slow dashboard performance, especially when switching between complex models like neural networks. Others found the layout of some visualizations (particularly LIME) overwhelming, and recommended simplifying visual elements or providing clearer sectioning. Suggestions included replacing generic index labels with more meaningful identifiers (e.g., “house #12”), adding personalization, and improving visual design polish.\n",
    "\n",
    "From this research, we derived the following key insights for improvement: users need a concise introduction or guide to help them navigate the dashboard and understand explanation methods; visual clutter, especially in LIME sections, should be reduced, with more intuitive labels and polished layouts; replacing index numbers with meaningful names (such as “House A” or “House in Amsterdam”) would improve comprehension; and faster model switching and reduced load times would enhance the fluidity of user interaction.\n",
    "\n",
    "The data analysis followed a thematic coding process. User responses were reviewed and categorized into themes: preferred explanation method, clarity and usefulness of visualizations, confusion points, and suggestions for improvement. SHAP emerged as the most preferred explanation method, praised for its intuitive layout, meaningful feature attribution, and ease of interpretation. In contrast, LIME received mixed feedback—some users appreciated its localized predictions, while others found it confusing or overwhelming due to the density of graphs and unclear terminology.\n",
    "\n",
    "A recurring issue was confusion regarding the lack of agreement between methods, particularly between SHAP and LIME, with users questioning which method to trust and why discrepancies occurred. There was a strong demand for onboarding support, such as brief textual descriptions, a user guide, or tooltips explaining the methods and their roles. Suggestions also included clearer labels, more interactive help elements, and personalization (e.g., labeling houses rather than using index numbers).\n",
    "\n",
    "Usability feedback highlighted minor interface issues, such as slow loading times, visual clutter, and a desire for more polished layout designs, especially for neural network explanations.\n",
    "\n",
    "In summary, the user research provided valuable insights into how moderately technical users engage with XAI tools and what barriers exist in understanding model behavior. The results confirmed the dashboard’s overall value, especially in enabling comparison and transparency, but also revealed critical usability and communication gaps. These insights are now guiding further iterations—focusing on simplifying LIME outputs, integrating layered explanations, and improving interaction design to enhance overall clarity and trust in AI predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6f6e1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- *Conclusion of the user testing*\n",
    "- *A well-argued description of the final prototype, including visualizations*\n",
    "- *Visualization of the interaction of the user with the concept in the use-context.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8bc16",
   "metadata": {},
   "source": [
    "# Short description of design archive\n",
    "\n",
    "- *Method overview, references to the archive that contains materials used in user research (e.g. probe materials, interview guide, observation scheme), the notes you took throughout your design process and collected RAW data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c52955c",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Centraal Bureau voor de Statistiek. (2024). Woningmarkt Dashboard. CBS. Retrieved April 26, 2025, from https://www.cbs.nl/nl-nl/visualisaties/dashboard-economie/woningmarkt\n",
    "- De Nadai, M., Staiano, J., Larcher, R., Sebe, N., Quercia, D., & Lepri, B. (2016). The death and life of great Italian cities: A mobile phone data perspective. arXiv preprint arXiv:1609.01845. https://arxiv.org/abs/1609.01845\n",
    "- Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874. https://arxiv.org/abs/1705.07874\n",
    "- Molnar, C. (2020). Interpretable machine learning: A guide for making black box models explainable (2nd ed.). https://christophm.github.io/interpretable-ml-book/\n",
    "- Özçelik, M. H., & Yildirim, S. (2022). Explainable artificial intelligence techniques in real estate valuation: A comparative analysis. Computers & Industrial Engineering, 174, 108039. https://doi.org/10.1016/j.cie.2022.108039\n",
    "- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). \"Why should I trust you?\": Explaining the predictions of any classifier. arXiv preprint arXiv:1602.04938. https://arxiv.org/abs/1602.04938\n",
    "- Smilkov, D., Thorat, N., Kim, B., Viégas, F., & Wattenberg, M. (2017). SmoothGrad: Removing noise by adding noise. arXiv preprint arXiv:1706.03825. https://arxiv.org/abs/1706.03825\n",
    "- Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365. https://arxiv.org/abs/1703.01365\n",
    "- Statistics Netherlands (CBS). (2024). Housing market reports. CBS Netherlands. https://www.cbs.nl/en-gb\n",
    "- Rabobank Research. (2024). Housing market analyses Netherlands. Rabobank Economics. https://economics.rabobank.com/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "C:\\Users\\HUAWEI\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
